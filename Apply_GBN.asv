% p :     % proportion of rows to select for training

function Acc=Apply_GBN(p, examples, labels)

[X_train,y_train, X_test,y_test]=SplitData(examples,labels)

% train a Naive Bayes classifier
   [classifier] = trainClassifier(X_train,y_train,'nbayes');   %train classifier

% apply the Naive Bayes classifier to the training data (it's best to use cross
% validation, of course, to obtain an estimate of its true error).  The returned
% array 'predictions' is an array where predictions(k,j) = log P(example_k |
% class_j).

   [y_predicted] = applyClassifier(X_test,classifier);       %test it

% summarize the results of the above predictions.   

 [result,predictedLabels,trace] = summarizePredictions(y_predicted,classifier,'averageRank',y_test);
 Acc=1-result{1};  % rank accuracy
 
 
function [X_train,y_train, X_test,y_test]=SplitData(X,y)

N = size(X,1);  % total number of rows 
tf = false(N,1) ;   % create logical index vector
tf(1:round(p*N)) = true ;    
tf = tf(randperm(N));   % randomise order
X_train = examples(tf,:) ;
y_train = labels(tf) ;

X_test = examples(~tf,:) ;
y_test = labels(~tf) ;